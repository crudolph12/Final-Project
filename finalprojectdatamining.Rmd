---
title: "Predicting Daily Volatility"
author:
- by Hyunpyo Kim, Milo Opdahl, Charles Rudolph
output: github_document
---
##Abstract
In this project we ask the following question: What if we could predict daily volatility of a company’s stock price and use that to our advantage?  We found significance in this question due to our likeminded interest in stock market swings and trends.  We created a model that could measure daily volatility using information from the S&P 500 Index and 50 individual companies (selected by market cap).  Using a volatility measure of the difference in the log of closing price from day “t” to day “t-1”, we ran lasso regressions to predict the day-ahead volatilities of the top five companies in the S&P 500 (Apple, Microsoft, Amazon, Facebook, and Google).  Using results from lasso, we were able to comment on relationships between the top 5 companies’ period t volatilities and the other companies’ t-1 volatilities, as well as form predictive models for the top 5.  We followed up by running a train-test split on our models, which showed that in reality our models had minimal support.  Although we did find a model that does a better job in predicting volatility, we did not have time to fully incorporate it into our project; however, there is some discussion of this new model in the appendix.  Ultimately, this project was an informative exercise into better understanding how difficult it is to predict volatility.

##Introduction
Many people get caught up in the impossible idea of conquering the stock market.  From the outside looking in, it appears to be a sure method of gaining a lot of money in a short amount of time.  In reality, the stock market is quite unpredictable in its movements.  A company’s stock may swing up or down in the blink of an eye,  making stock direction and price seem impossible to peg for the future.  However, what if we could predict the daily volatility of a company’s stock price and use that to our advantage?  

On a surface level, volatility may seem worthless: it only measures how much a stock could move in the future, without pinpointing the direction of the movement.  However, it is an important measurement in the option’s market, as traders analyze it in order to determine risk level in a particular transaction.  For example, high volatility allows for riskier trading opportunities, while low volatility leads to safer but less profitable trades.  

Given this, we deem it necessary to be able to create a model that effectively measures volatility.  In particular, we will predict day-ahead volatility for individual stocks utilizing yesterday’s volatility for a variety of stocks, with stocks being drawn from the S&P 500.  Companies will be ranked by market cap.  From there, we will predict day-ahead volatility for the top five companies in this category (Apple, Microsoft, Amazon, Facebook, Google).  The remaining companies in the dataset will comprise of the rest of the top 50 companies by market cap.  Additionally, we will analyze which individual companies daily volatilities from period t-1 have the strongest effects on each of the top five companies’ daily volatility from period t.  Daily volatility will be defined in a simplistic way: the difference in the log of closing price from day “t” to day “t-1”.  This is a well-established measurement often utilized by stock market experts.  Lastly, we will attempt to observe the viability of our daily volatility models.


##Methods
Naturally, an immense amount of financial data was required to create a suitable model.  Using the library “Quantmod” in R, we were able to draw data from December 31, 2015 to May 30, 2019 for the 50 companies’ stocks we desired.  Within the datasets, numerous daily variables were included, such as opening price, closing price, volume of stock traded, high price, and low price.  A “daily volatility” variable was defined using the definition from the introduction.  Additionally, we constructed a dataset for Apple, Microsoft, Amazon, Facebook, and Google that compared daily volatility for period “t” to daily volatility for period “t-1” for the rest of the companies.  

Once this was organized, we opted to utilize a lasso regression between each of our five companies and the top 50 companies in the S&P 500.  We believe that using a regularization technique such as lasso regression was the best option for three reasons.  First, using regularization will reduce any overfitting in our stock market data by adding a penalty weight to the fitted betas we wish to find.  This will allow for more realism regarding the movements of stock prices for our data.  Second, the lasso regression gives sparse solutions in its output.  This means that only the most important fitted betas that are not identically zero will be considered through an automatic variable selection.  Third,  lasso regression performs well when the number of features are small compared to the number of observations.  For our stock market dataset, there are certainly many more observations than there are features due to the time-period and the number of companies being observed.  To use lasso regression, we utilized the library “gamlr” in R.  This library was the best option for lasso regression analysis since it allowed some flexibility in applying model selection rules to our code.  After successfully running lasso on the five key companies,  we used the beta values to fit a line to the daily volatility plots for the five key companies, which served as our predictive model.  (Note: lasso regression plots can be observed in the appendix, figure 5.)

To best understand whether or not our predictive model was viable, we needed to test it under a controlled environment.  This was done by doing a  train-test split on the predicted models of the top five companies created with the lasso regressions.  The train-test split method allows us to see how well out model would perform in an out-of-sample prediction.  It is our hope that we are able to find a root means-squared error (RMSE) in our prediction that is lower than the control RMSE.

##Results

```{r include=FALSE}
library(quantmod)
library(gamlr)
library(tidyverse)
library(tibble)
library(dplyr)
library(scales)
library(ggpubr)
library(png)
options(digits = 4)

startDate = as.Date("2015-12-31")
endDate = as.Date("2019-04-30") 

SP500C = read.csv("~/Downloads/SP500c.csv", head = TRUE)
SP500_sym = c("AAPL","MSFT", "AMZN", "FB", "JPM", "GOOG",
              "XOM", "V", "BAC", "PG", "DIS", "VZ", "CSCO", "CVX", "UNH",
              "PFE", "MA", "T", "HD", "INTC", "MRK", "CMCSA", "WFC", "BA",
              "KO", "PEP", "C", "NFLX", "MCD", "WMT", "ORCL", "ADBE", "ABT",
              "PM", "PYPL", "UNP", "HON", "IBM", "AVGO", "CRM", "MDT", "ABBV",
              "ACN", "UTX", "TMO", "AMGN", "TXN", "QCOM", "NVDA", "MMM") 

# loading all SP500 stock data, and combine
ST = getSymbols(SP500_sym[1], from = startDate, to = endDate, auto.assign = F)
colnames(ST) = c("open","high","low","close","volume","adjusted")
ST$Volatility = (diff(log(ST[,4]), lag = 1))
ST = ST[-1,]
sp500 = data.frame(date = index(ST), stock = SP500_sym[1], ST)
for (i in 2:length(SP500_sym)) {
  ST = getSymbols(SP500_sym[i], from = startDate, to = endDate, auto.assign = F)
  colnames(ST) = c("open","high","low","close","volume","adjusted")
  ST$Volatility = (diff(log(ST[,4]), lag = 1))
  ST = ST[-1,]
  ST = data.frame(date = index(ST), stock = SP500_sym[i], ST)
  sp500 = rbind(sp500, ST)
}
#plot any companys vol that you desire
#apple
ST = getSymbols(SP500_sym[1], from = startDate, to = endDate, auto.assign = F)
colnames(ST) = c("open","high","low","close","volume","adjusted")
ST$Volatility = (diff(log(ST[,4]), lag = 1))
ST = ST[-1,]
appledv=ST %>%
  as.data.frame() %>%
  rownames_to_column("Date") %>%
  mutate(Date = as.Date(Date, format = "%Y-%m-%d")) %>%
  ggplot(aes(Date, Volatility)) +
  geom_line() +
  scale_x_date(
    date_breaks = "6 month",
    labels = date_format("%b\n%Y")) +
  theme_minimal() +
  ggtitle("Apple Daily Volatility")
#Microsoft
ST = getSymbols(SP500_sym[2], from = startDate, to = endDate, auto.assign = F)
colnames(ST) = c("open","high","low","close","volume","adjusted")
ST$Volatility = (diff(log(ST[,4]), lag = 1))
ST = ST[-1,]
microsoftdv=ST %>%
  as.data.frame() %>%
  rownames_to_column("Date") %>%
  mutate(Date = as.Date(Date, format = "%Y-%m-%d")) %>%
  ggplot(aes(Date, Volatility)) +
  geom_line() +
  scale_x_date(
    date_breaks = "6 month",
    labels = date_format("%b\n%Y")) +
  theme_minimal() +
  ggtitle("Microsoft Daily Volatility")
#Amazon
ST = getSymbols(SP500_sym[3], from = startDate, to = endDate, auto.assign = F)
colnames(ST) = c("open","high","low","close","volume","adjusted")
ST$Volatility = (diff(log(ST[,4]), lag = 1))
ST = ST[-1,]
amazondv=ST %>%
  as.data.frame() %>%
  rownames_to_column("Date") %>%
  mutate(Date = as.Date(Date, format = "%Y-%m-%d")) %>%
  ggplot(aes(Date, Volatility)) +
  geom_line() +
  scale_x_date(
    date_breaks = "6 month",
    labels = date_format("%b\n%Y")) +
  theme_minimal() +
  ggtitle("Amazon Daily Volatility")
#Facebook
ST = getSymbols(SP500_sym[4], from = startDate, to = endDate, auto.assign = F)
colnames(ST) = c("open","high","low","close","volume","adjusted")
ST$Volatility = (diff(log(ST[,4]), lag = 1))
ST = ST[-1,]
fbdv=ST %>%
  as.data.frame() %>%
  rownames_to_column("Date") %>%
  mutate(Date = as.Date(Date, format = "%Y-%m-%d")) %>%
  ggplot(aes(Date, Volatility)) +
  geom_line() +
  scale_x_date(
    date_breaks = "6 month",
    labels = date_format("%b\n%Y")) +
  theme_minimal() +
  ggtitle("Facebook Daily Volatility")
#Google
ST = getSymbols(SP500_sym[6], from = startDate, to = endDate, auto.assign = F)
colnames(ST) = c("open","high","low","close","volume","adjusted")
ST$Volatility = (diff(log(ST[,4]), lag = 1))
ST = ST[-1,]
googledv=ST %>%
  as.data.frame() %>%
  rownames_to_column("Date") %>%
  mutate(Date = as.Date(Date, format = "%Y-%m-%d")) %>%
  ggplot(aes(Date, Volatility)) +
  geom_line() +
  scale_x_date(
    date_breaks = "6 month",
    labels = date_format("%b\n%Y")) +
  theme_minimal() +
  ggtitle("Google Daily Volatility")
```

####Figure 1: Daily Volatility
```{r include=FALSE}
figure1=ggarrange(appledv, microsoftdv, amazondv, fbdv, googledv, ncol=2, nrow=3)
annotate_figure(figure1, bottom= text_grob("Figure 1", hjust=1, x=1))
```

```{r echo=FALSE}
knitr::include_graphics("https://raw.githubusercontent.com/crudolph12/hw3/master/figure1.png")
```

Figure 1 displays the daily volatility for the previously established “top 5 companies” that we are tracking.  For any given company during the chosen period, volatility rarely exceeds 10%.  Apple appears to be the most volatile stock on a day-to-day basis.

```{r include=FALSE}
#begin lasso
apple = getSymbols("AAPL", from = startDate, to = endDate, auto.assign = F)
SP500vols = apple$AAPL.Open

for (i in 1:length(SP500_sym)){
  STC = getSymbols(SP500_sym[i], from = startDate, to = endDate, auto.assign = F)
  VOL = (diff(log(STC[,4]), lag = 1))
  #VOL = volatility(STC, n = 5, calc = 'close', N = 250)
  colnames(VOL) = SP500_sym[i]
  SP500vols = cbind(SP500vols, VOL)
  
}
SP500vols = as.matrix(SP500vols[-nrow(SP500vols),-1])
AAPL = (diff(log(apple[,4]), lag=1))
AAPL = AAPL[-1,]
AAPL = data.frame(APPL_crt=AAPL, SP500vols[])
AAPL=AAPL[-1,]
AAPLx=as.matrix(AAPL[,-1])
AAPLy=AAPL$AAPL.Close

AAPLlasso=gamlr(AAPLx,AAPLy,family="gaussian")
plot(AAPLlasso)
AICc(AAPLlasso)
plot(AAPLlasso$lambda, AICc(AAPLlasso))
AAPLbeta=coef(AAPLlasso)
AAPLbeta

Volcomp=as.data.frame((as.matrix(AAPLbeta)))
colnames(Volcomp)[colnames(Volcomp)=="seg26"]<-"AAPL.T"

microsoft = getSymbols("MSFT", from = startDate, to = endDate, auto.assign = F)
MSFT = (diff(log(microsoft[,4]), lag=1))
MSFT = MSFT[-1,]
MSFT = data.frame(MSFT_crt=MSFT, SP500vols[])
MSFT=MSFT[-1,]
MSFTx=as.matrix(MSFT[,-1])
MSFTy=MSFT$MSFT.Close

MSFTlasso=gamlr(MSFTx,MSFTy,family="gaussian")
plot(MSFTlasso)
AICc(MSFTlasso)
plot(MSFTlasso$lambda, AICc(MSFTlasso))
MSFTbeta=coef(MSFTlasso)
MSFTbeta

VolcompMSFT=as.data.frame((as.matrix(MSFTbeta)))
colnames(VolcompMSFT)[colnames(VolcompMSFT)=="seg38"]<-"MSFT.T"
Volcompcombined<-cbind(Volcomp,VolcompMSFT)

amazon = getSymbols("AMZN", from = startDate, to = endDate, auto.assign = F)
AMZN = (diff(log(amazon[,4]), lag=1))
AMZN = AMZN[-1,]
AMZN = data.frame(AMZN_crt=AMZN, SP500vols[])
AMZN=AMZN[-1,]
AMZNx=as.matrix(AMZN[,-1])
AMZNy=AMZN$AMZN.Close

AMZNlasso=gamlr(AMZNx,AMZNy,family="gaussian")
plot(AMZNlasso)
AICc(AMZNlasso)
plot(AMZNlasso$lambda, AICc(AMZNlasso))
AMZNbeta=coef(AMZNlasso)
AMZNbeta

VolcompAMZN=as.data.frame((as.matrix(AMZNbeta)))
colnames(VolcompAMZN)[colnames(VolcompAMZN)=="seg27"]<-"AMZN.T"
Volcompcombined1<-cbind(Volcompcombined,VolcompAMZN)

#facebook FB
facebook = getSymbols("FB", from = startDate, to = endDate, auto.assign = F)
FB = (diff(log(facebook[,4]), lag=1))
FB = FB[-1,]
FB = data.frame(FB_crt=FB, SP500vols[])
FB=FB[-1,]
FBx=as.matrix(FB[,-1])
FBy=FB$FB.Close

FBlasso=gamlr(FBx,FBy,family="gaussian")
plot(FBlasso)
AICc(FBlasso)
plot(FBlasso$lambda, AICc(FBlasso))
FBbeta=coef(FBlasso)
FBbeta

VolcompFB=as.data.frame((as.matrix(FBbeta)))
colnames(VolcompFB)[colnames(VolcompFB)=="seg28"]<-"FB.T"
Volcompcombined2<-cbind(Volcompcombined1,VolcompFB)

#google GOOG
google = getSymbols("GOOG", from = startDate, to = endDate, auto.assign = F)
GOOG = (diff(log(google[,4]), lag=1))
GOOG = GOOG[-1,]
GOOG = data.frame(GOOG_crt=GOOG, SP500vols[])
GOOG=GOOG[-1,]
GOOGx=as.matrix(GOOG[,-1])
GOOGy=GOOG$GOOG.Close

GOOGlasso=gamlr(GOOGx,GOOGy,family="gaussian")
plot(GOOGlasso)
AICc(GOOGlasso)
plot(GOOGlasso$lambda, AICc(GOOGlasso))
GOOGbeta=coef(GOOGlasso)
GOOGbeta

VolcompGOOG=as.data.frame((as.matrix(GOOGbeta)))
colnames(VolcompGOOG)[colnames(VolcompGOOG)=="seg32"]<-"GOOG.T"
Volcompcombined3<-cbind(Volcompcombined2,VolcompGOOG)
#Predictive lines
#Apple
AAPL$date = rownames(AAPL)
AAPLpredictive=.0008+.0716*(AAPL$PM)+.0625*(AAPL$GOOG)+.0160*(AAPL$ACN)-
  .0332*(AAPL$PFE)-.0366*(AAPL$MCD)-.0463*(AAPL$UTX)-.0510*(AAPL$WFC)-
  .0583*(AAPL$VZ)-.0666*(AAPL$XOM)
#Microsoft
MSFT$date=rownames(MSFT)
MSFTpredictive=.0011+.0835*(MSFT$GOOG)+.0771*(MSFT$PEP)+.0476*(MSFT$HD)+
  .0439*(MSFT$PM)+.0428*(MSFT$PYPL)+.0357*(MSFT$FB)-.0012*(MSFT$INTC)-
  .0076*(MSFT$CRM)-.0166*(MSFT$NFLX)-.0167*(MSFT$CSCO)-.0208*(MSFT$BAC)-
  .0432*(MSFT$VZ)-.0492*(MSFT$T)-.0504*(MSFT$UTX)-.1228*(MSFT$PFE)-.1574*(MSFT$MSFT)
#Amazon
AMZN$date=rownames(AMZN)
AMZNpredictive=.0014+.1277*(AMZN$HD)+.0697*(AMZN$PEP)+.0590*(AMZN$TMO)-
  .0059*(AMZN$XOM)-.0065*(AMZN$AVGO)-.0118*(AMZN$IBM)-.0172*(AMZN$MMM)-
  .0212*(AMZN$VZ)-.0260*(AMZN$MSFT)-.0265*(AMZN$INTC)-.0340*(AMZN$WFC)-
  .0377*(AMZN$MRK)-.0540*(AMZN$WMT)-.0577*(AMZN$UTX)-.0719*(AMZN$CVX)
#FB
FB$date=rownames(FB)
FBpredictive=.0010+.0990*(FB$GOOG)+.0840*(FB$PM)+.0830*(FB$HD)+.0240*(FB$PEP)-
  .0018*(FB$QCOM)-.0052*(FB$AMGN)-.0133*(FB$NFLX)-.0186*(FB$UNP)-.0369*(FB$MRK)-
  .0388*(FB$HON)-.0737*(FB$MSFT)-.0856*(FB$CSCO)-.0863*(FB$AAPL)
#Google
GOOG$date=rownames(GOOG)
GOOGpredictive=.0007+.0769*(GOOG$GOOG)+.0698*(GOOG$PM)+.0450*(GOOG$KO)+.0352*(GOOG$PYPL)+
  .0280*(GOOG$PEP)+.0219*(GOOG$HD)-.0027*(GOOG$AMZN)-.0029*(GOOG$BA)-.0041*(GOOG$WMT)-
  .0138*(GOOG$CSCO)-.0180*(GOOG$PFE)-.0193*(GOOG$AMGN)-.0210*(GOOG$IBM)-.0276*(GOOG$AAPL)-
  .0290*(GOOG$T)-.0301*(GOOG$DIS)-.0345*(GOOG$XOM)-.0732*(GOOG$MSFT)-.0950*(GOOG$UTX)
```

```{r include=FALSE}
#Combine all the plots into one image space.
layout(matrix(c(1,1,2,3,4,5), 3, 2, byrow=TRUE))
plot(strptime(AAPL$date,"%Y-%m-%d"),AAPL$AAPL.Close,type="l",
     xlab="Date",ylab="Volatility", main="Predicted Volatility for Apple")
lines(strptime(AAPL$date,"%Y-%m-%d"),AAPLpredictive, col="blue")
plot(strptime(MSFT$date,"%Y-%m-%d"),MSFT$MSFT.Close,type="l",
     xlab="Date",ylab="Volatility", main="Predicted Volatility for Microsoft")
lines(strptime(MSFT$date,"%Y-%m-%d"),MSFTpredictive, col="blue")
plot(strptime(AMZN$date,"%Y-%m-%d"),AMZN$AMZN.Close,type="l",
     xlab="Date",ylab="Volatility", main="Predicted Volatility for Amazon")
lines(strptime(AMZN$date,"%Y-%m-%d"),AMZNpredictive, col="blue")
plot(strptime(FB$date,"%Y-%m-%d"),FB$FB.Close,type="l",
     xlab="Date",ylab="Volatility", main="Predicted Volatility for Facebook")
lines(strptime(FB$date,"%Y-%m-%d"),FBpredictive, col="blue")
plot(strptime(GOOG$date,"%Y-%m-%d"),GOOG$GOOG.Close,type="l",
     xlab="Date",ylab="Volatility", main="Predicted Volatility for Google")
lines(strptime(GOOG$date,"%Y-%m-%d"),GOOGpredictive, col="blue")
layout(matrix(c(1), 1, 1, byrow=TRUE))
```

```{r echo=FALSE}
Company_Rankings <- tibble::tribble(
  ~Company, ~Rank1, ~Rank2, ~Rank3, ~Rank4, ~Rank5,
      "Apple",     "Philip_Morris",     "Exxon_Mobil(-)",     "Google",     "Verizon(-)",     "Wells_Fargo(-)",
      "Microsoft",     "Microsoft(-)",     "Pfizer(-)",     "Google",     "PepsiCo",  "United_Technologies(-)",
      "Amazon",     "Home_Depot",     "Chevron(-)",     "PepsiCo",     "Thermo_Fisher",     "United_Technologies(-)",
      "Facebook",     "Google",     "Apple(-)",     "Cisco(-)",     "Philip_Morris",     "Home_Depot",
      "Google",     "United_Technologies(-)",     "Google",     "Microsoft(-)",     "Philip_Morris",     "Coca_Cola" )
```

####Table 1: Top 5 Most Influential "t-1 Volatilities" on top 5 companies' "t Volatilities"
```{r echo=FALSE}
require(knitr)
kable(Company_Rankings, digits = 3, row.names = FALSE, align = "c",
              caption = NULL)
```

The table depicts which companies’ t-1 volatilities best predict the top 5 companies’ volatilities in period t.  Companies are listed in order of most predictive to least.  Predictive power was determined via analyzing the beta values from lasso, and picking the biggest 5 in terms of absolute value.  Company names with a negative sign next to them signal an inverse relationship between t-1 and t volatilities.

####Figure 2: Predicted Daily Volatility
```{r echo=FALSE}
knitr::include_graphics("https://raw.githubusercontent.com/crudolph12/hw3/master/doit.png")
```

Figure 2 displays a predictive line running through the daily volatility.  This line was formed by multiplying the beta values obtained from lasso by the t-1 volatilities pertaining to each company with a significant beta.

##Conclusion

The table and figures above paint an interesting picture when analyzing daily volatility for the top 5 companies.  Figure 1 did a good job of allowing us to visualize just how volatile a stock could be from day to day.  It seemed that on an average day volatility could vary by as much as 1 percent, while on a noteworthy day in the company’s history volatility could swing by 10 to 20 percent.  As a result of the average days severely outnumbering the shocking days, t-1 volatility from the top 50 companies seems to always predict low volatility days for period t.  As can be noted from figure 2, the predicted daily volatility seems to hover around a 1% change, rarely predicting any significant movement.  This shortcoming of our model can be explained by the variables we chose to predict with: t-1 volatilities from the 50 companies has no way to account for surprise events that will happen tomorrow.  However, predicted volatility did fit daily volatility for Facebook rather well , especially from late 2017 to early 2018.  During this period, Facebook’s daily volatility was very low.  So, it appears our model can do a great job of predicting stocks with generally low volatilities.

The table provides insight into which companies’ t-1 volatilities the lasso determined were most predictive of the top 5 companies’ t volatilities.  In general, it seemed companies with established relationships or that resided in similar industries predicted each other well.  For example, t-1 daily volatility for Google was the best predictor for t daily volatility for Facebook.  Given that they are two of the most popular websites in the world, it is only natural that their two’s volatilities are intertwined.  Another interesting point was the common occurrence of inverse relationships between the top 5 and top 50 companies’ volatilities.  An inverse relationship meant that as the t-1 volatility for one of the 50 companies increased/decreased, the predicted t volatility of the top 5 company in question moved in the opposite direction.  An example of this occurred when discussing the relationship between Apple and Exxon Mobil.  One potential explanation for this may be that extreme volatility in an energy market such as Exxon Mobil could leave the public in a financially unstable place, resulting in less purchases of the often expensive technology that Apple provides.  In turn, fewer purchases could feasibly lower daily volatility.  Additionally, further consideration of the observed inverse volatility relationships should remind us of some aspects of correlation analysis.  One thought is that correlation does not necessarily imply causation.  Indeed, the previously mentioned volatility relationship between Apple and Exxon Mobil may not necessarily have a direct link to one another.  Perhaps there are other factors that we have not considered in our model that explain why the volatility of these two companies happen to be inverse.

Two strange results stood out to us: Microsoft’s inverse relationship with itself, and the prevalence of Philip Morris.  Both are difficult to interpret and fuel suspicion towards our model’s legitimacy.  Perhaps the inverse relationship could be explained by Microsoft stock trying to correct towards a certain benchmark, such as 0% volatility.  Philip Morris’ constant presence in the rankings is easier to explain.  By observing the daily volatility of the company (seen in the appendix, figure 3), it is clear that Philip Morris is a company with minimal daily volatility.  This indicates the model may be tending towards keeping daily volatility predictions low, giving credence to discussions of the same topic earlier in the conclusion.

Given the often strange results of our table, we found it imperative to run train test splits on our data.  We created a test split that featured 2019 data, and proceeded to run k-fold lasso regression on the splits on each of the top 5 companies.  Unsurprisingly, we found very few meaningful results.  It appeared that the usage of t-1 company volatility was a poor predictor of t volatility.  In the big picture, this certainly makes sense.  As stated earlier in the report, anything involving stock would be near impossible to predict.  Such a simplistic model was bound to run into its fair share of issues.  Certainly, there are a multitude of other factors that could possibly explain the daily volatility of companies in the S&P 500.  However, we do not feel the project was a failure.  Our measure of daily volatility from figure 1 are useful in their own right, and a few of the relationships featured in the table seemed legitimate.

Feeling inspired to create at least one suitable model, we were able to form a predictive model involving daily volatility drawing from more conventional methods that exceeded expectations for train test splits and k-fold lasso regression.  Sadly, we did not have time to conduct a proper analysis and interpretation of the new model; however, perhaps a future in-depth inquiry to this model would be of use for us when considering daily volatility.  This model can be viewed in the appendix, Figure 4.

Although we were unsuccessful in creating a perfect model, this project was useful in showing the difficulties faced when trying to create a model that would predict daily volatility.  We hope that in future considerations, we may be able to use our new model to better interpret how volatility takes place in the stock market.

##Appendix

####Figure 3:Philip Morris Daily Volatility

```{r include=FALSE}
ST = getSymbols(SP500_sym[34], from = startDate, to = endDate, auto.assign = F)
colnames(ST) = c("open","high","low","close","volume","adjusted")
ST$Volatility = (diff(log(ST[,4]), lag = 1))
```

```{r include=FALSE}
plot(ST$Volatility, main= "Daily Volatility of Philip Morris")
```

```{r echo=FALSE}
knitr::include_graphics("https://raw.githubusercontent.com/crudolph12/hw3/master/figure3.png")
```

This figure confirms the hypothesis that Philip Morris has minimal daily volatility throughout the time period chosen for the project, lending credence to the idea that our predictive model prefered a low, constant volatility.

####Figure 4: Conventianal Model for predicting Daily Volatility

Our new model focuses on using conventional predictive variables to estimate daily volatility.  In order to form an effective model, we opted to utilize a train test split, with the train drawing from 2016-2018 data and the test attempting to predict 2019 data.  Our measurement of daily volatility will stay the same as our initial experiment, (log(close(t))/log(close(t-1))).

The variables we decided to use are as follows: 

1. stock’s price and volume data : open, high, low. close, volume and adjusted price
2. Moving average of closing price(5 days, 50 days, 200 days) :  
    * difference between MA line and closing price : log(closing price(t)/MA50(t))
    * daily moving trends of MA line : log(MA50(t)/(MA50(t-1))
3. Golden Cross/Death Cross signal (the point where 50 days MA line and 200 days MA line crosses)
4. Historical high and low : the highest and lowest closing price within the past 52 weeks (1 year), the proportional closing price within the highest and lowest price
5. Candlestick : the gap between high and low price, and the gap between open and close price
6. Historical volatility : 21 days and 5 days historical volatility ( close to close method)
7. VIX : high and close, drawn from the VIX
8. Industry : The daily volatilities of stocks in the same industry
9. Major indices : S&P500, Dow30, NASDAQ, Russell 2000
10. ETF for world market :  world(without US), South America, China, Japan, Hong Kong, India, Germany, UK, France
12. Meaningful days of week : Monday and Friday, which experience different trends than other days.

After establishing this list, we ran a lasso regression on the train test split for many companies.  As expected, our model can’t always explain the stock’s volatility.  Often, lasso models selected by cross validation contained only the intercept, meaning the average would be better than the model we formed.  However, some stocks were able to be predicted effectively.  A specific example of this was the predictive model for Microsoft.

```{r include=FALSE}
library(quantmod)
library(tidyverse)


# the list of S&P500 companies
SP500C = read.csv("~/Downloads/SP500c.csv", head = TRUE)

# the target stock to predict
stock = "MSFT"

# stocks in the same industry as the target stock
indst_S = SP500C[which(SP500C[,1]==stock),4]
indst_C = as.character(SP500C[which(SP500C[,4]==indst_S),1])

# analysis period
startDateT = as.Date("2014-12-31") # for some historical dataset
startDate = as.Date("2015-12-30")
endDate = as.Date("2019-04-30")

# target stock
Xvar = getSymbols(stock, from = startDateT, to = endDate, auto.assign = F)

## target volatility
v = log(Xvar[,4]) %>% diff(lag = 1)
colnames(v) = c("target Vol")

## trending : 5d/50d/200d
MA5 = Cl(Xvar) %>% SMA(n = 5)
MA5_Cl = log(Xvar[,4]/MA5)
MA5_tr = log(MA5) %>% diff(lag = 1)
MA50 = Cl(Xvar) %>% SMA(n = 50)
MA50_Cl = log(Xvar[,4]/MA50)
MA50_tr = log(MA50) %>% diff(lag = 1)
MA200 = Cl(Xvar) %>% SMA(n = 200)
MA200_Cl = log(Xvar[,4]/MA200)
MA200_tr = log(MA200) %>% diff(lag = 1)

## trending : signal of golden cross/death cross
MA_G_sig = ifelse((MA50 >= MA200)&(lag(MA50,1)<lag(MA200,1)), 1, 0)
MA_D_sig = ifelse((MA50 <= MA200)&(lag(MA50,1)>lag(MA200,1)), 1, 0)

## trending : current price level considering 52weeks high and low
H52W = runMax(Xvar[,4], n = 250, cumulative = FALSE)
L52W = runMin(Xvar[,4], n = 250, cumulative = FALSE)
HL52W = (Xvar[,4]-L52W)/(H52W-L52W)

## candle stick 
HL = (Xvar[,2]-Xvar[,3])/Xvar[,1]
OC = (Xvar[,4]-Xvar[,1])/Xvar[,1]

## historical volatility
hvol21 = volatility(Xvar, n = 21, calc = 'close', N = 250)
hvol5 = volatility(Xvar, n = 5, calc = 'close', N = 250)


## combine all data for the stock
Xvar = cbind(Xvar, MA5_Cl, MA5_tr, MA50_Cl, MA50_tr, MA200_Cl, MA200_tr, 
             MA_G_sig, MA_D_sig, H52W, L52W, HL52W, HL, OC, hvol21, hvol5)
colnames(Xvar) = c("open", "high", "low", "close", "volume", "adusted",
                   "MA_5d", "MA_5d_sl", "MA_50d", "MA_50d_sl", "MA_200d", "MA_200d_sl",
                   "G_sig", "D_sig", "High_52w","Low_52w", "High&Low_52w", 
                   "Candle_HighLow", "Candle_OpenClose", "h_vol_21d", "h_vol_5d")


# VIX : high and closing
getSymbols("^VIX", from = startDate, to = endDate, auto.assign = T)
Xvar = cbind(Xvar, VIX[,c(2,4)])


# list of crawling : industry, S&P500, Dow30, NASDAQ, Russell 2000
# index : world(w/o US), southAmerica, china, japan, hongkong, india, germany, UK, france  
addX = c(indst_C, "^GSPC", "^DJI", "^IXIC", "^RUT", 
         "VEU", "ILF", "GXC", "EWJ", "EWH", "INDY", "EWG", "EWU", "EWQ")

for (i in 1:length(addX)){
  CRWL = getSymbols(addX[i], from = startDate, to = endDate, auto.assign = F)
  RTN = ROC(Cl(CRWL))
  colnames(RTN) = addX[i]
  Xvar = cbind(Xvar, RTN)
}

# cut the X variable from 2015-12-31 to 2019-4-26 because those are yesterday data
Xvar = Xvar["2015-12-31/2019-04-29"]

# check NA
which(is.na(Xvar) == "TRUE")

# combine target y variable and X variable(laggin 1 day for yesterday analysis)
for (i in 1:ncol(Xvar)){
  lagged_X = lag(Xvar[,i], 1)
  v = cbind(v,lagged_X)  
}
v = v["2016-01-01/2019-04-29"]

# combine days of week
days = weekdays(index(v))
v$mon = ifelse(days=="Monday", 1, 0)
v$fri = ifelse(days=="Friday", 1, 0)


# making model & test

## function for testing 
### RMSE
RMSE = function(y, yhat){
  sqrt( mean( (y - yhat)^2 ) )
}


## split data

X_train = v["2016-01-01/2018-12-13",-1] %>% as.matrix()
Y_train = v["2016-01-01/2018-12-13",1]
X_test = v["2019",-1] %>% as.matrix()
Y_test = v["2019",1] %>% as.matrix()


## LASSO
library(gamlr)

### AICs + lasso
lasso = gamlr(X_train, Y_train, family = "gaussian")
beta = coef(lasso)
Yhat = predict(lasso, newdata = X_test)
sum(beta!=0)

### CV + lasso
cv_lasso = cv.gamlr(X_train, Y_train, nfold = 10, family = "gaussian" ,verb=TRUE)
cv_beta = coef(cv_lasso, select="min")
YCVhat = predict(cv_lasso, newdata = X_test, select="min")
sum(cv_beta!=0)

result = cbind(Y_test, Yhat, YCVhat)
result = xts(result, order.by = as.POSIXct(rownames(result)))
```


```{r include=FALSE}
## plotting
par(mar=c(5,5,1,1))
plot(as.zoo(result), screens = 1, lty = c(1,2,2), ylim = c(-0.05, 0.05),
     col = c("black","red","blue"), xlab = "Date (2019", ylab = "Volatility")
grid(nx = 100, ny = 10)
legend("bottomright", legend=c("Volatility", "Volatility_hat1", "Volatility_hat2"),
       col = c("black","red","blue"), lty = c(1,2,2), cex = 0.5)
```

```{r echo=FALSE}
knitr::include_graphics("https://raw.githubusercontent.com/crudolph12/hw3/master/figure4.png")
```


The plot above displays daily volatility in black, predicted daily volatility via lasso in red(y_hat1) and predicted daily volatility via cross validated lasso in blue (y_hat2).  It seems that the predicted lines do a respectable job of predicting volatility.  The RMSE values reveal that the lasso model selected by cross validation is better than that selected by AIC and the model for 0.

```{r}
c(RMSE(Y_test,Yhat), RMSE(Y_test,YCVhat), RMSE(Y_test,0))
```

####Figure 5: Lasso Regression Plots for the top 5 companies
```{r echo=FALSE}
knitr::include_graphics("https://raw.githubusercontent.com/crudolph12/hw3/master/lasso.png")
```

The dotted vertical line represents the optimal level for log lambda, which minimizes AICc.  The number of colored lines intersecting this represent the number of betas significant at the given lambda value.  This method was how we determined which betas would be utilized in our predictive model for daily volatility, as seen in figure 2.